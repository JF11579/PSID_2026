{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXP4ZjKZmZsp6PF+buRhuu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oHJhk4uu--z-"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","═══════════════════════════════════════════════════════════════════════════════\n","PSID MASTER PIPELINE - COMPLETE DATA PROCESSING\n","═══════════════════════════════════════════════════════════════════════════════\n","\n","This script processes PSID data from raw files to analysis-ready datasets.\n","\n","Research Question:\n","Does G1 (1968) homeownership affect G2 and G3 outcomes (homeownership, education)?\n","\n","Pipeline Stages:\n","1. Load raw data (FIMS + PSID)\n","2. Create unique person IDs\n","3. Link generations (G1 → G2 → G3)\n","4. Extract homeownership (V103: 1=Own, 5=Rent, 8=Other)\n","5. Harmonize education across waves\n","6. Build network structure for visualization\n","7. Export clean datasets\n","\n","Outputs:\n","- analysis_ready_data.csv (for regression analysis)\n","- family_network_nodes.csv (for visualization)\n","- family_network_edges.csv (for visualization)\n","- summary_statistics.txt (data quality report)\n","\n","Author: Your work + Claude's integration\n","Date: January 2025\n","═══════════════════════════════════════════════════════════════════════════════\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# CONFIGURATION\n","# =============================================================================\n","\n","# Mount Google Drive (for Colab)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Paths\n","DATA_DIR = \"/content/drive/MyDrive/DATA/PSID_data\"\n","OUTPUT_DIR = DATA_DIR  # Save outputs in same directory\n","\n","# Input files\n","FIMS_FILE = f\"{DATA_DIR}/FIMS_Beth.csv\"\n","PSID_FILE = f\"{DATA_DIR}/J355167.csv\"\n","LABELS_FILE = f\"{DATA_DIR}/J355167_labels.txt\"\n","\n","# Output files\n","OUTPUT_ANALYSIS = f\"{OUTPUT_DIR}/analysis_ready_data.csv\"\n","OUTPUT_NODES = f\"{OUTPUT_DIR}/family_network_nodes.csv\"\n","OUTPUT_EDGES = f\"{OUTPUT_DIR}/family_network_edges.csv\"\n","OUTPUT_SUMMARY = f\"{OUTPUT_DIR}/pipeline_summary.txt\"\n","\n","print(\"=\"*80)\n","print(\" PSID MASTER PIPELINE - COMPLETE DATA PROCESSING \".center(80))\n","print(\"=\"*80)\n","print()\n","\n","# =============================================================================\n","# STAGE 1: LOAD DATA\n","# =============================================================================\n","\n","print(\"STAGE 1: LOADING DATA\")\n","print(\"-\"*80)\n","\n","# Load FIMS\n","print(f\"Loading FIMS from: {FIMS_FILE}\")\n","fims = pd.read_csv(FIMS_FILE)\n","print(f\"✓ Loaded {len(fims):,} rows × {fims.shape[1]} columns\")\n","\n","# Load PSID\n","print(f\"Loading PSID from: {PSID_FILE}\")\n","psid_raw = pd.read_csv(PSID_FILE)\n","print(f\"✓ Loaded {len(psid_raw):,} rows × {psid_raw.shape[1]} columns\")\n","\n","print()\n","\n","# =============================================================================\n","# STAGE 2: CREATE PERSON IDs\n","# =============================================================================\n","\n","print(\"STAGE 2: CREATING UNIQUE PERSON IDs\")\n","print(\"-\"*80)\n","\n","# PSID person_id: ER30001 * 1000 + ER30002\n","psid_raw['ER30001'] = pd.to_numeric(psid_raw['ER30001'], errors='coerce').astype('Int64')\n","psid_raw['ER30002'] = pd.to_numeric(psid_raw['ER30002'], errors='coerce').astype('Int64')\n","psid_raw.insert(0, 'person_id', psid_raw['ER30001'] * 1000 + psid_raw['ER30002'])\n","\n","n_unique_persons = psid_raw['person_id'].nunique()\n","print(f\"✓ Created person_id for {n_unique_persons:,} unique individuals\")\n","\n","# FIMS IDs\n","for col in ['G1ID68', 'G1PN', 'G2ID68', 'G2PN']:\n","    if col in fims.columns:\n","        fims[col] = pd.to_numeric(fims[col], errors='coerce').astype('Int64')\n","\n","fims.insert(0, 'grandparent_id', fims['G1ID68'] * 1000 + fims['G1PN'])\n","fims.insert(1, 'parent_id', fims['G2ID68'] * 1000 + fims['G2PN'])\n","\n","# Check for G3\n","if 'G3ID68' in fims.columns and 'G3PN' in fims.columns:\n","    fims['G3ID68'] = pd.to_numeric(fims['G3ID68'], errors='coerce').astype('Int64')\n","    fims['G3PN'] = pd.to_numeric(fims['G3PN'], errors='coerce').astype('Int64')\n","    fims.insert(2, 'child_id', fims['G3ID68'] * 1000 + fims['G3PN'])\n","    has_g3 = True\n","else:\n","    has_g3 = False\n","\n","n_g1 = fims['grandparent_id'].nunique()\n","n_g2 = fims[fims['parent_id'].notna()]['parent_id'].nunique()\n","\n","print(f\"✓ G1 (grandparents): {n_g1:,} unique individuals\")\n","print(f\"✓ G2 (parents): {n_g2:,} unique individuals\")\n","\n","if has_g3:\n","    n_g3 = fims[fims['child_id'].notna()]['child_id'].nunique()\n","    print(f\"✓ G3 (children): {n_g3:,} unique individuals\")\n","\n","print()\n","\n","# =============================================================================\n","# STAGE 3: EXTRACT HOMEOWNERSHIP (V103)\n","# =============================================================================\n","\n","print(\"STAGE 3: EXTRACTING HOMEOWNERSHIP\")\n","print(\"-\"*80)\n","\n","# V103 codes: 1=Own, 5=Rent, 8=Other\n","if 'V103' in psid_raw.columns:\n","    print(\"Using V103 for 1968 homeownership\")\n","    print(\"Codes: 1=Own, 5=Rent, 8=Other\")\n","\n","    # Create binary homeownership indicator\n","    psid_raw['homeowner_1968'] = psid_raw['V103'].apply(\n","        lambda x: 1 if x == 1 else (0 if x == 5 else np.nan)\n","    )\n","\n","    # Keep the raw V103 for reference\n","    psid_raw['v103_raw'] = psid_raw['V103']\n","\n","    # Summary\n","    v103_dist = psid_raw['V103'].value_counts(dropna=False)\n","    print(f\"\\nV103 Distribution:\")\n","    print(f\"  Own (1):   {v103_dist.get(1.0, 0):>8,} ({v103_dist.get(1.0, 0)/len(psid_raw)*100:>5.1f}%)\")\n","    print(f\"  Rent (5):  {v103_dist.get(5.0, 0):>8,} ({v103_dist.get(5.0, 0)/len(psid_raw)*100:>5.1f}%)\")\n","    print(f\"  Other (8): {v103_dist.get(8.0, 0):>8,} ({v103_dist.get(8.0, 0)/len(psid_raw)*100:>5.1f}%)\")\n","    print(f\"  Missing:   {psid_raw['V103'].isna().sum():>8,} ({psid_raw['V103'].isna().sum()/len(psid_raw)*100:>5.1f}%)\")\n","\n","    homeowner_dist = psid_raw['homeowner_1968'].value_counts(dropna=False)\n","    print(f\"\\nBinary Homeowner:\")\n","    print(f\"  Owner:   {homeowner_dist.get(1, 0):>8,}\")\n","    print(f\"  Renter:  {homeowner_dist.get(0, 0):>8,}\")\n","    print(f\"  Missing: {psid_raw['homeowner_1968'].isna().sum():>8,}\")\n","else:\n","    print(\"⚠ WARNING: V103 not found!\")\n","    psid_raw['homeowner_1968'] = np.nan\n","    psid_raw['v103_raw'] = np.nan\n","\n","print()\n","\n","# =============================================================================\n","# STAGE 4: HARMONIZE EDUCATION\n","# =============================================================================\n","\n","print(\"STAGE 4: HARMONIZING EDUCATION\")\n","print(\"-\"*80)\n","\n","# Education variables (most recent first)\n","EDU_VARS = [\n","    'ER35152',  # 2023\n","    'ER34952',  # 2021\n","    'ER34934',  # 2019\n","    'ER34930',  # 2019\n","    'ER34734',  # 2017\n","    'ER34730',  # 2017\n","    'ER34534',  # 2015\n","    'ER34530',  # 2015\n","    'ER34335',  # 2013\n","    'ER34331',  # 2013\n","    'ER33817',  # 2011\n","    'ER30657',  # 1989\n","    'ER30584',  # 1985\n","]\n","\n","# Check which education variables exist\n","edu_vars_present = [v for v in EDU_VARS if v in psid_raw.columns]\n","print(f\"Found {len(edu_vars_present)} education variables:\")\n","for var in edu_vars_present:\n","    n_valid = psid_raw[var].notna().sum()\n","    print(f\"  {var}: {n_valid:>8,} valid values\")\n","\n","# Create harmonized education variable (use most recent non-null)\n","if edu_vars_present:\n","    psid_raw['education_years'] = psid_raw[edu_vars_present].bfill(axis=1).iloc[:, 0]\n","\n","    # Classify into categories\n","    def classify_education(years):\n","        \"\"\"Classify education into interpretable categories\"\"\"\n","        if pd.isna(years):\n","            return 'Missing'\n","        elif years < 12:\n","            return 'HS Dropout'\n","        elif years == 12:\n","            return 'HS Grad'\n","        elif 13 <= years < 16:\n","            return 'Some College'\n","        else:\n","            return 'College Grad'\n","\n","    psid_raw['education_level'] = psid_raw['education_years'].apply(classify_education)\n","\n","    # Summary\n","    print(f\"\\nHarmonized Education Summary:\")\n","    print(f\"  Valid: {psid_raw['education_years'].notna().sum():,}\")\n","    print(f\"  Missing: {psid_raw['education_years'].isna().sum():,}\")\n","    print(f\"\\nEducation Level Distribution:\")\n","    print(psid_raw['education_level'].value_counts())\n","else:\n","    print(\"⚠ WARNING: No education variables found!\")\n","    psid_raw['education_years'] = np.nan\n","    psid_raw['education_level'] = 'Missing'\n","\n","print()\n","\n","# =============================================================================\n","# STAGE 5: LINK GENERATIONS\n","# =============================================================================\n","\n","print(\"STAGE 5: LINKING GENERATIONS\")\n","print(\"-\"*80)\n","\n","# Create lookup tables for merging\n","print(\"Creating lookup tables...\")\n","\n","# G1 lookup (grandparent attributes)\n","g1_vars = ['person_id', 'homeowner_1968', 'v103_raw', 'ER32000', 'ER32006']\n","g1_vars = [v for v in g1_vars if v in psid_raw.columns]\n","g1_lookup = psid_raw[g1_vars].drop_duplicates('person_id').copy()\n","g1_lookup.columns = ['grandparent_id'] + [f'g1_{c}' for c in g1_lookup.columns[1:]]\n","\n","# G2/G3 lookup (education and demographics)\n","person_vars = ['person_id', 'education_years', 'education_level', 'ER32000', 'ER32006']\n","person_vars = [v for v in person_vars if v in psid_raw.columns]\n","person_lookup = psid_raw[person_vars].drop_duplicates('person_id').copy()\n","\n","# Start with FIMS as the base (all family links)\n","merged = fims[['grandparent_id', 'parent_id']].dropna().drop_duplicates().copy()\n","print(f\"Starting with {len(merged):,} G1→G2 links\")\n","\n","# Merge G1 attributes\n","merged = merged.merge(g1_lookup, on='grandparent_id', how='left')\n","print(f\"✓ Merged G1 attributes\")\n","\n","# Merge G2 attributes\n","g2_lookup = person_lookup.copy()\n","g2_lookup.columns = ['parent_id'] + [f'g2_{c}' for c in g2_lookup.columns[1:]]\n","merged = merged.merge(g2_lookup, on='parent_id', how='left')\n","print(f\"✓ Merged G2 attributes\")\n","\n","# Add G3 if available\n","if has_g3:\n","    g2_g3_links = fims[['parent_id', 'child_id']].dropna().drop_duplicates()\n","\n","    # Create a separate G2→G3 dataset\n","    g2_g3_merged = g2_g3_links.copy()\n","\n","    # Add G2 attributes (parent info)\n","    g2_g3_merged = g2_g3_merged.merge(g2_lookup, on='parent_id', how='left')\n","\n","    # Add G3 attributes\n","    g3_lookup = person_lookup.copy()\n","    g3_lookup.columns = ['child_id'] + [f'g3_{c}' for c in g3_lookup.columns[1:]]\n","    g2_g3_merged = g2_g3_merged.merge(g3_lookup, on='child_id', how='left')\n","\n","    # Add grandparent link (G1→G2→G3)\n","    g1_g2_for_g3 = merged[['grandparent_id', 'parent_id', 'g1_homeowner_1968']].drop_duplicates()\n","    g2_g3_merged = g2_g3_merged.merge(g1_g2_for_g3, on='parent_id', how='left')\n","\n","    print(f\"✓ Created G2→G3 dataset with {len(g2_g3_merged):,} links\")\n","\n","print(f\"\\nFinal merged data: {len(merged):,} rows\")\n","print()\n","\n","# =============================================================================\n","# STAGE 6: CREATE ANALYSIS-READY DATASETS\n","# =============================================================================\n","\n","print(\"STAGE 6: CREATING ANALYSIS-READY DATASETS\")\n","print(\"-\"*80)\n","\n","# Dataset 1: G1→G2 Analysis (Does G1 homeownership predict G2 education?)\n","analysis_g1_g2 = merged.copy()\n","\n","# Add birth year if available (for cohort controls)\n","if 'ER30004' in psid_raw.columns:\n","    birth_lookup = psid_raw[['person_id', 'ER30004']].copy()\n","    birth_lookup.columns = ['parent_id', 'g2_birth_year']\n","    analysis_g1_g2 = analysis_g1_g2.merge(birth_lookup, on='parent_id', how='left')\n","    # Calculate birth decade\n","    analysis_g1_g2['g2_birth_decade'] = (analysis_g1_g2['g2_birth_year'] // 10) * 10\n","\n","# Clean column names for analysis\n","analysis_g1_g2.rename(columns={\n","    'g1_homeowner_1968': 'parent_homeowner',\n","    'g2_education_years': 'child_education_years',\n","    'g2_education_level': 'child_education_level',\n","    'g2_ER32000': 'child_sex',\n","    'g2_ER32006': 'child_race',\n","    'g1_ER32000': 'parent_sex',\n","    'g1_ER32006': 'parent_race'\n","}, inplace=True)\n","\n","# Remove rows with missing key variables\n","analysis_g1_g2_clean = analysis_g1_g2[\n","    analysis_g1_g2['parent_homeowner'].notna() &\n","    analysis_g1_g2['child_education_years'].notna()\n","].copy()\n","\n","print(f\"G1→G2 Analysis Dataset:\")\n","print(f\"  Total rows: {len(analysis_g1_g2):,}\")\n","print(f\"  Complete cases: {len(analysis_g1_g2_clean):,}\")\n","print(f\"  Homeowners: {(analysis_g1_g2_clean['parent_homeowner']==1).sum():,}\")\n","print(f\"  Renters: {(analysis_g1_g2_clean['parent_homeowner']==0).sum():,}\")\n","\n","# Dataset 2: G1→G2→G3 Analysis (if G3 exists)\n","if has_g3:\n","    analysis_g1_g2_g3 = g2_g3_merged.copy()\n","\n","    analysis_g1_g2_g3.rename(columns={\n","        'g1_homeowner_1968': 'grandparent_homeowner',\n","        'g2_education_years': 'parent_education_years',\n","        'g2_education_level': 'parent_education_level',\n","        'g3_education_years': 'child_education_years',\n","        'g3_education_level': 'child_education_level',\n","        'g3_ER32000': 'child_sex',\n","        'g3_ER32006': 'child_race'\n","    }, inplace=True)\n","\n","    analysis_g1_g2_g3_clean = analysis_g1_g2_g3[\n","        analysis_g1_g2_g3['grandparent_homeowner'].notna() &\n","        analysis_g1_g2_g3['child_education_years'].notna()\n","    ].copy()\n","\n","    print(f\"\\nG1→G2→G3 Analysis Dataset:\")\n","    print(f\"  Total rows: {len(analysis_g1_g2_g3):,}\")\n","    print(f\"  Complete cases: {len(analysis_g1_g2_g3_clean):,}\")\n","\n","print()\n","\n","# =============================================================================\n","# STAGE 7: BUILD NETWORK FOR VISUALIZATION\n","# =============================================================================\n","\n","print(\"STAGE 7: BUILDING NETWORK STRUCTURE\")\n","print(\"-\"*80)\n","\n","# Create node list (all unique individuals)\n","nodes_list = []\n","\n","# G1 nodes\n","g1_nodes = pd.DataFrame({\n","    'node_id': fims['grandparent_id'].unique(),\n","    'generation': 'Gen1'\n","})\n","g1_attrs = psid_raw[['person_id', 'homeowner_1968', 'ER32000', 'ER32006']].drop_duplicates('person_id')\n","g1_nodes = g1_nodes.merge(g1_attrs, left_on='node_id', right_on='person_id', how='left')\n","g1_nodes.drop(columns=['person_id'], inplace=True)\n","g1_nodes['education_level'] = 'Unknown'  # G1 education not typically tracked\n","nodes_list.append(g1_nodes)\n","\n","# G2 nodes\n","g2_nodes = pd.DataFrame({\n","    'node_id': fims['parent_id'].dropna().unique(),\n","    'generation': 'Gen2'\n","})\n","g2_attrs = psid_raw[['person_id', 'education_level', 'ER32000', 'ER32006']].drop_duplicates('person_id')\n","g2_nodes = g2_nodes.merge(g2_attrs, left_on='node_id', right_on='person_id', how='left')\n","g2_nodes.drop(columns=['person_id'], inplace=True)\n","g2_nodes['homeowner_1968'] = np.nan  # G2 homeownership would need separate variable\n","nodes_list.append(g2_nodes)\n","\n","# G3 nodes (if available)\n","if has_g3:\n","    g3_nodes = pd.DataFrame({\n","        'node_id': fims['child_id'].dropna().unique(),\n","        'generation': 'Gen3'\n","    })\n","    g3_attrs = psid_raw[['person_id', 'education_level', 'ER32000', 'ER32006']].drop_duplicates('person_id')\n","    g3_nodes = g3_nodes.merge(g3_attrs, left_on='node_id', right_on='person_id', how='left')\n","    g3_nodes.drop(columns=['person_id'], inplace=True)\n","    g3_nodes['homeowner_1968'] = np.nan\n","    nodes_list.append(g3_nodes)\n","\n","# Combine all nodes\n","all_nodes = pd.concat(nodes_list, ignore_index=True)\n","print(f\"Created network nodes: {len(all_nodes):,}\")\n","print(all_nodes['generation'].value_counts())\n","\n","# Create edge list\n","edges_list = []\n","\n","# G1→G2 edges\n","g1_g2_edges = fims[['grandparent_id', 'parent_id']].dropna().drop_duplicates()\n","g1_g2_edges.columns = ['source', 'target']\n","edges_list.append(g1_g2_edges)\n","\n","# G2→G3 edges\n","if has_g3:\n","    g2_g3_edges = fims[['parent_id', 'child_id']].dropna().drop_duplicates()\n","    g2_g3_edges.columns = ['source', 'target']\n","    edges_list.append(g2_g3_edges)\n","\n","# Combine all edges\n","all_edges = pd.concat(edges_list, ignore_index=True)\n","print(f\"Created network edges: {len(all_edges):,}\")\n","\n","print()\n","\n","# =============================================================================\n","# STAGE 8: EXPORT DATASETS\n","# =============================================================================\n","\n","print(\"STAGE 8: EXPORTING DATASETS\")\n","print(\"-\"*80)\n","\n","# Export analysis-ready data\n","analysis_g1_g2.to_csv(OUTPUT_ANALYSIS, index=False)\n","print(f\"✓ Saved: {OUTPUT_ANALYSIS}\")\n","print(f\"  Rows: {len(analysis_g1_g2):,}\")\n","\n","# Export G1→G2→G3 if exists\n","if has_g3:\n","    output_g3 = OUTPUT_ANALYSIS.replace('.csv', '_g1_g2_g3.csv')\n","    analysis_g1_g2_g3.to_csv(output_g3, index=False)\n","    print(f\"✓ Saved: {output_g3}\")\n","    print(f\"  Rows: {len(analysis_g1_g2_g3):,}\")\n","\n","# Export network data for visualization\n","all_nodes.to_csv(OUTPUT_NODES, index=False)\n","print(f\"✓ Saved: {OUTPUT_NODES}\")\n","print(f\"  Nodes: {len(all_nodes):,}\")\n","\n","all_edges.to_csv(OUTPUT_EDGES, index=False)\n","print(f\"✓ Saved: {OUTPUT_EDGES}\")\n","print(f\"  Edges: {len(all_edges):,}\")\n","\n","print()\n","\n","# =============================================================================\n","# STAGE 9: GENERATE SUMMARY REPORT\n","# =============================================================================\n","\n","print(\"STAGE 9: GENERATING SUMMARY REPORT\")\n","print(\"-\"*80)\n","\n","summary_lines = []\n","summary_lines.append(\"=\"*80)\n","summary_lines.append(\"PSID DATA PROCESSING SUMMARY\")\n","summary_lines.append(\"=\"*80)\n","summary_lines.append(\"\")\n","summary_lines.append(\"INPUT FILES:\")\n","summary_lines.append(f\"  FIMS: {FIMS_FILE}\")\n","summary_lines.append(f\"  PSID: {PSID_FILE}\")\n","summary_lines.append(\"\")\n","summary_lines.append(\"DATA QUALITY:\")\n","summary_lines.append(f\"  Total individuals: {n_unique_persons:,}\")\n","summary_lines.append(f\"  G1 (grandparents): {n_g1:,}\")\n","summary_lines.append(f\"  G2 (parents): {n_g2:,}\")\n","if has_g3:\n","    summary_lines.append(f\"  G3 (children): {n_g3:,}\")\n","summary_lines.append(\"\")\n","summary_lines.append(\"HOMEOWNERSHIP (V103):\")\n","summary_lines.append(f\"  Owners: {(psid_raw['homeowner_1968']==1).sum():,}\")\n","summary_lines.append(f\"  Renters: {(psid_raw['homeowner_1968']==0).sum():,}\")\n","summary_lines.append(f\"  Missing: {psid_raw['homeowner_1968'].isna().sum():,}\")\n","summary_lines.append(\"\")\n","summary_lines.append(\"EDUCATION:\")\n","summary_lines.append(f\"  Valid: {psid_raw['education_years'].notna().sum():,}\")\n","summary_lines.append(f\"  Missing: {psid_raw['education_years'].isna().sum():,}\")\n","summary_lines.append(\"\")\n","summary_lines.append(\"ANALYSIS DATASETS:\")\n","summary_lines.append(f\"  G1→G2 complete cases: {len(analysis_g1_g2_clean):,}\")\n","if has_g3:\n","    summary_lines.append(f\"  G1→G2→G3 complete cases: {len(analysis_g1_g2_g3_clean):,}\")\n","summary_lines.append(\"\")\n","summary_lines.append(\"OUTPUT FILES:\")\n","summary_lines.append(f\"  {OUTPUT_ANALYSIS}\")\n","if has_g3:\n","    summary_lines.append(f\"  {output_g3}\")\n","summary_lines.append(f\"  {OUTPUT_NODES}\")\n","summary_lines.append(f\"  {OUTPUT_EDGES}\")\n","summary_lines.append(\"\")\n","summary_lines.append(\"=\"*80)\n","\n","# Save to file\n","with open(OUTPUT_SUMMARY, 'w') as f:\n","    f.write('\\n'.join(summary_lines))\n","\n","# Print to console\n","print('\\n'.join(summary_lines))\n","\n","print(f\"\\n✓ Saved summary: {OUTPUT_SUMMARY}\")\n","print()\n","\n","# =============================================================================\n","# PIPELINE COMPLETE\n","# =============================================================================\n","\n","print(\"=\"*80)\n","print(\" PIPELINE COMPLETE \".center(80))\n","print(\"=\"*80)\n","print()\n","print(\"✅ All datasets created successfully!\")\n","print()\n","print(\"NEXT STEPS:\")\n","print(\"  1. Review the summary report\")\n","print(\"  2. Run the analysis script: ANALYSIS_INTERGENERATIONAL_EFFECTS.py\")\n","print(\"  3. Create visualizations using the network files\")\n","print()\n","print(\"=\"*80)"]}]}